{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "resnet-implementation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ReanSchwarzer1/Asia-Uni-Final-Group2-Project/blob/main/resnet_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zovBYSozHTJ"
      },
      "source": [
        "# Asia University Winter Program '21 Group 2 Project\n",
        "\n",
        "## Complimentary notebook to try to use ResNet architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "uczIL0lFzHTK",
        "outputId": "459e8348-35ed-4bc6-f0c1-99a2c2bd6fb9"
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/kaggle/input/age-gender-and-ethnicity-face-data-csv/age_gender.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5oDxNq8azHTM"
      },
      "source": [
        "data = pd.read_csv('/kaggle/input/age-gender-and-ethnicity-face-data-csv/age_gender.csv')\n",
        "data['pixels'] = data.pixels.apply(lambda x: x.split(' '))\n",
        "data['pixels'] = data.pixels.apply(lambda x: np.array([int(v) for v in x]))\n",
        "data['pixels'] = data.pixels.apply(lambda x: x.reshape(48,48))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "utFzmKSizHTM"
      },
      "source": [
        "import os # accessing directory structure\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt # plotting\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from  IPython.display import display\n",
        "import plotly.express as px\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, experimental, MaxPool2D, BatchNormalization\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy, binary_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import test\n",
        "import random\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BQMUO3-wzHTN"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(data.drop(['age','ethnicity','gender','img_name'], axis=1),\n",
        "                                                  data[['age','ethnicity','gender']], random_state=0, test_size=0.25)\n",
        "\n",
        "\n",
        "def preprocess (df, y):\n",
        "    \"\"\"Redim df\"\"\"\n",
        "    X = np.zeros((len(df.values), 48, 48, 1))\n",
        "    for idx,array in enumerate(df[y]):\n",
        "        X[idx, :, :, 0] = array\n",
        "    return X\n",
        "\n",
        "# We expand dimension to fit with the CNN inputs\n",
        "Xtrain = preprocess(X_train, 'pixels')\n",
        "Xval = preprocess(X_val, 'pixels')\n",
        "\n",
        "# We decided to make prediction only on age but it can easily be done on the other \n",
        "ytrain = y_train.age.values\n",
        "yval = y_val.age.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjtE5J-GzHTO"
      },
      "source": [
        "## SE-RESNET Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_4jiJOUzHTO"
      },
      "source": [
        "We are implementing a residual bloc and an SE block combined together which can make really good predictions.\n",
        "\n",
        "A SE block is not looking for spatial patterns like CNN, it learns the caracteristics which work well in group. Like nose and mouth are relatively close on a face the NN will expect to see eyes. If it constats a high activation for the nose and mouth feature cards and a medium one for the eyes, the block will excite the last one.\n",
        "\n",
        "A block SE has only 3 layers and pulls out a vector which will multiply the feature cards of a previous resnet block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "tIeD-FT2zHTP"
      },
      "source": [
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [keras.layers.Conv2D(filters, 3, strides=strides, padding=\"same\",use_bias=False),\n",
        "                            keras.layers.BatchNormalization(), # Normalize the outputs\n",
        "                            self.activation,\n",
        "                            keras.layers.Conv2D(filters, 3, strides=1, padding='same', use_bias=False),\n",
        "                            keras.layers.BatchNormalization()]\n",
        "        self.skip_layers = [\n",
        "            keras.layers.Conv2D(filters, 1, strides=strides,padding=\"same\",use_bias=False),\n",
        "            keras.layers.BatchNormalization()\n",
        "        ]\n",
        "    \n",
        "    # We don't forget the call method which is called during the training and prediction\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)\n",
        "    \n",
        "class SEBloc(keras.layers.Layer):\n",
        "    def __init__(self, pool, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.main_layers = [keras.layers.AveragePooling2D(\n",
        "                                pool_size=pool, strides=1, padding=\"same\"), # pool_size is important, we need a scalar per feature card\n",
        "                            keras.layers.Dense(5, activation='relu'), # embedding\n",
        "                            keras.layers.Dense(64, activation='sigmoid')] # outputs \n",
        "    \n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        return Z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sihXacL2zHTP"
      },
      "source": [
        "## SENET model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oTxARD-VzHTQ"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i8oS3V26zHTR"
      },
      "source": [
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "inputs = tf.keras.Input(shape=(48,48,1), dtype=\"float32\")\n",
        "x = keras.layers.Conv2D(64, 5, strides=2, input_shape=[48,48,1])(inputs)\n",
        "x = keras.layers.BatchNormalization()(x)\n",
        "x = keras.layers.Activation(\"relu\")(x)\n",
        "x = keras.layers.MaxPool2D(pool_size=2, strides=2, padding='same')(x)\n",
        "\n",
        "x_res = ResidualUnit(64, strides=1)(x) # RES\n",
        "\n",
        "x_se = SEBloc(x.shape[1])(x) #SE\n",
        "\n",
        "x_res_se = keras.layers.Multiply()([x_res, x_se]) # Multiply outputs of SE and RES\n",
        "x = keras.layers.Add()([x_res_se, x])\n",
        "\n",
        "x = keras.layers.Dropout(0.5)(x)\n",
        "x = keras.layers.Flatten()(x)\n",
        "output = keras.layers.Dense(1, activation='relu')(x) # One output with relu for the regression\n",
        "model = tf.keras.Model(inputs, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7agpRQ76zHTR"
      },
      "source": [
        "## Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OKKXqTaUzHTS",
        "outputId": "f4482cf5-2bcd-4a99-c438-37ab6c1f26a6"
      },
      "source": [
        "class ExponentialLearningRate(keras.callbacks.Callback):\n",
        "    \n",
        "    def __init__(self, K, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "        self.K = K\n",
        "        \n",
        "    def on_batch_end(self, batch, logs):\n",
        "        \n",
        "        self.rates.append(self.K.get_value(self.model.optimizer.lr))\n",
        "        self.losses.append(logs[\"loss\"])\n",
        "        self.K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
        "        \n",
        "        \n",
        "def bestLearningRate():\n",
        "        \n",
        "        print(\"\\n\\n********************** Learning rate calculation ******************\\n\\n\")\n",
        "        K = keras.backend\n",
        "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "        expon_lr = ExponentialLearningRate(K,factor=1.0003)\n",
        "        model.fit(Xtrain, ytrain, validation_data=(Xval, yval), epochs = 20, callbacks=[expon_lr])\n",
        "        print(\"*************************************************************************\\n\\n\")\n",
        "        \n",
        "        print(\"********************** Loss as function of learning rate plot displayed ********************\\n\\n\")\n",
        "        \n",
        "        fig = px.line(\n",
        "        x=expon_lr.rates, y=expon_lr.losses,\n",
        "        labels={'index': 'learning rate', 'value': 'loss'}, \n",
        "        title='Training History')\n",
        "        fig.show()\n",
        "        \n",
        "        id_min = np.argmin(expon_lr.losses)\n",
        "        return expon_lr.rates[id_min]\n",
        "        \n",
        "lr = bestLearningRate()\n",
        "print('the learning rate is: ',lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "********************** Learning rate calculation ******************\n",
            "\n",
            "\n",
            "Epoch 1/20\n",
            "556/556 [==============================] - 34s 60ms/step - loss: 291.4025 - mean_absolute_error: 12.9176 - val_loss: 306.4839 - val_mean_absolute_error: 14.7766\n",
            "Epoch 2/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 151.6863 - mean_absolute_error: 9.3068 - val_loss: 220.3962 - val_mean_absolute_error: 10.8263\n",
            "Epoch 3/20\n",
            "556/556 [==============================] - 33s 59ms/step - loss: 132.0051 - mean_absolute_error: 8.6252 - val_loss: 287.4597 - val_mean_absolute_error: 14.1021\n",
            "Epoch 4/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 126.5314 - mean_absolute_error: 8.4083 - val_loss: 426.4958 - val_mean_absolute_error: 17.9101\n",
            "Epoch 5/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 122.3401 - mean_absolute_error: 8.3072 - val_loss: 668.2391 - val_mean_absolute_error: 22.9683\n",
            "Epoch 6/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 115.8454 - mean_absolute_error: 8.0719 - val_loss: 310.0596 - val_mean_absolute_error: 14.6615\n",
            "Epoch 7/20\n",
            "556/556 [==============================] - 33s 60ms/step - loss: 110.6597 - mean_absolute_error: 7.9609 - val_loss: 1056.8862 - val_mean_absolute_error: 29.1362\n",
            "Epoch 8/20\n",
            "556/556 [==============================] - 34s 61ms/step - loss: 110.4683 - mean_absolute_error: 7.9286 - val_loss: 409.2979 - val_mean_absolute_error: 16.6035\n",
            "Epoch 9/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 104.0284 - mean_absolute_error: 7.6330 - val_loss: 177.7400 - val_mean_absolute_error: 10.1328\n",
            "Epoch 10/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 102.2466 - mean_absolute_error: 7.5994 - val_loss: 257.9960 - val_mean_absolute_error: 12.9123\n",
            "Epoch 11/20\n",
            "556/556 [==============================] - 32s 57ms/step - loss: 101.5812 - mean_absolute_error: 7.5925 - val_loss: 492.6073 - val_mean_absolute_error: 18.8048\n",
            "Epoch 12/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 100.6145 - mean_absolute_error: 7.5744 - val_loss: 276.4811 - val_mean_absolute_error: 14.0306\n",
            "Epoch 13/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 104.5032 - mean_absolute_error: 7.6781 - val_loss: 623.6165 - val_mean_absolute_error: 21.5313\n",
            "Epoch 14/20\n",
            "556/556 [==============================] - 32s 58ms/step - loss: 103.1656 - mean_absolute_error: 7.6390 - val_loss: 361.4227 - val_mean_absolute_error: 15.9295\n",
            "Epoch 15/20\n",
            "556/556 [==============================] - 32s 57ms/step - loss: 102.3406 - mean_absolute_error: 7.6353 - val_loss: 304.9124 - val_mean_absolute_error: 14.3179\n",
            "Epoch 16/20\n",
            " 48/556 [=>............................] - ETA: 26s - loss: 132.9647 - mean_absolute_error: 8.7013"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wve5sXCzHTS"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mnSIIVD3zHTT"
      },
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if(logs.get('val_loss')<91):\n",
        "            print(\"\\nReached 110 val_loss so cancelling training!\")\n",
        "            self.model.stop_training = True\n",
        "        \n",
        "callback = myCallback()\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "SGD = tf.keras.optimizers.Adam(learning_rate=0.0035) \n",
        "model.compile(loss='mse', optimizer=SGD ,metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=5, validation_data=(Xval, yval), callbacks = [callback, reduce_lr], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_10IFrsFzHTT"
      },
      "source": [
        "## Loss graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5CNFZUsGzHTT"
      },
      "source": [
        "fig = px.line(\n",
        "    history.history, y=['loss', 'val_loss'],\n",
        "    labels={'index': 'epoch', 'value': 'loss'}, \n",
        "    title='Training History')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}